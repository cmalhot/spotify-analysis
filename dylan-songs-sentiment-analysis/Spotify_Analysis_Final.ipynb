{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import spotipy  # use terminal to pip install PyLyrics\n",
    "import numpy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# setup with spotify API\n",
    "# find client_id etc. post creating your app on spotify\n",
    "username='your-uname'\n",
    "client_id = 'your-client-id'\n",
    "client_secret = 'your-client-secret'\n",
    "\n",
    "# update the settings on the app page of spotify to get uri, scope etc.\n",
    "# use those credentials for calling data from spotify's API\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "\n",
    "# set spotify client credentials to sp to be used for further coding\n",
    "# accessing tracks and such, check spotipy documentation for reference:\n",
    "# https://spotipy.readthedocs.io/en/2.9.0/#api-reference\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# selected artist for analysis - Bob Dylan \n",
    "# want just the artist Bob Dylan from a list of artists matching the name Bob or Dylan \n",
    "# use uri specific to Bob Dylan at position 0 of the list \n",
    "name = \"Bob Dylan\" #chosen artist\n",
    "result = sp.search(q='artist:' + name, type='artist') #search query\n",
    "#result[\"artists\"][\"items\"][0]\n",
    "artist_uri = result[\"artists\"][\"items\"][0][\"uri\"] #'spotify:artist:74ASZWbe4lXaubB36ztrGX'\n",
    "\n",
    "#set limit and offset - need to loop through to get the entire list of tracks/albums for Bob Dylan \n",
    "limit=50\n",
    "offset=0\n",
    "# Store artist's albums' uris in a list\n",
    "album_uris = []\n",
    "\n",
    "\n",
    "# Pull all of the artist's albums\n",
    "sp_albums = sp.artist_albums(artist_uri, limit=limit, offset=offset)\n",
    "\n",
    "for i in range(len(sp_albums['items'])):\n",
    "    album_uris.append(sp_albums['items'][i]['uri'])   \n",
    "\n",
    "while len(album_uris) < sp_albums[\"total\"] :\n",
    "    offset+=limit\n",
    "    sp_albums = sp.artist_albums(artist_uri, limit=limit, offset=offset)\n",
    "\n",
    "    for i in range(len(sp_albums['items'])):\n",
    "        album_uris.append(sp_albums['items'][i]['uri'])\n",
    "album_uris\n",
    "\n",
    "#album_uris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(album_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all album tracks using the uri\n",
    "\n",
    "def albumSongs(uri):\n",
    "    album = uri #assign album uri to a_name\n",
    "    tracks = sp.album_tracks(album) #pull data on album tracks\n",
    "    for n in range(len(tracks['items'])): #for each song track\n",
    "        artist_tracks[tracks['items'][n]['uri']] = {}\n",
    "\n",
    "        \n",
    "artist_tracks = {}\n",
    "\n",
    "for i in album_uris: #each album\n",
    "    albumSongs(i)\n",
    "artist_tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(album_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tracks' release dates from the albums \n",
    "track_names=[]\n",
    "track_album_release_dates = []\n",
    "for i in artist_tracks:\n",
    "    track = sp.track(i)\n",
    "    track_names.append(track[\"name\"])\n",
    "    track_album_release_dates.append(track[\"album\"][\"release_date\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes\n",
    "import pandas as pd\n",
    "dat = pd.DataFrame()\n",
    "dat['track_name'] = track_names\n",
    "dat['release_date'] = track_album_release_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore live, remix and deluxe album versions\n",
    "# Credit:\n",
    "mask = [('live' not in s.lower() and 'deluxe' not in s.lower()\n",
    "         and 'remix' not in s.lower() and 'rmx' not in s.lower()\n",
    "        and 'remastered' not in s.lower() and 'take 2' not in s.lower() and \n",
    "        'take 3' not in s.lower() and 'take 4' not in s.lower() and \n",
    "        'take 5' not in s.lower() and 'take 6' not in s.lower() and \n",
    "         'take 7' not in s.lower() and 'take 8' not in s.lower()\n",
    "        and 'take 11' not in s.lower() and 'take 13' not in s.lower()) for s in dat.track_name.values]\n",
    "dat = dat[mask]\n",
    "dat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more packages \n",
    "from PyLyrics import *  # use terminal to pip install PyLyrics\n",
    "import re\n",
    "import nltk\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lyrics from tracks iterating over each track \n",
    "track_lyrics = []\n",
    "for i in dat.track_name.values:\n",
    "    try:\n",
    "        lyrics = PyLyrics.getLyrics('Bob Dylan',i)\n",
    "        track_lyrics.append(lyrics)\n",
    "    except:\n",
    "        # sometimes this may not work (e.g. songs recorded live do not have lyrics stored)\n",
    "        track_lyrics.append('exception')\n",
    "dat['lyrics'] = track_lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore exceptions\n",
    "mask = [('exception' != s.lower()) for s in dat.lyrics.values]\n",
    "dat = dat[mask]\n",
    "dat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new dataset\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with text analysis - data munging \n",
    "# clean the lyrics and make a corpus for each album\n",
    "\n",
    "# convert pd to csv \n",
    "bob_dylan_lyrics = dat.to_csv(\"~/Case_Studies/lyrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv \n",
    "bob_dylan_lyrics = pd.read_csv(\"~/Case_Studies/lyrics.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "\n",
    "def remove_punctuation(doc):\n",
    "    return punctuation.sub('', doc.lower())\n",
    "\n",
    "\n",
    "bob_dylan_lyrics['processed_text'] = np.vectorize(remove_punctuation)(bob_dylan_lyrics['lyrics'])\n",
    "\n",
    "# remove stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for index, row in bob_dylan_lyrics.iterrows():\n",
    "    bob_dylan_lyrics.at[index, 'processed_text'] = ' '.join([term for term in bob_dylan_lyrics.loc[index, 'processed_text'].split()\n",
    "                                                      if term not in stop_words])\n",
    "\n",
    "# stem\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "for index, row in bob_dylan_lyrics.iterrows():\n",
    "    bob_dylan_lyrics.at[index, 'processed_text'] = ' '.join([porter.stem(term) for term\n",
    "                                                      in bob_dylan_lyrics.loc[index, 'processed_text'].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "bob_dylan_lyrics['tokenized'] = ''\n",
    "bob_dylan_lyrics['tokenized'] = bob_dylan_lyrics['tokenized'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write new csv post text parsing\n",
    "bob_dylan_lyrics.to_csv('/Users/Chandni/Documents/Education/NCSU/MSA/Case_Studies/lyrics_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages for creating word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column to use for word cloud \n",
    "fields = ['processed_text']\n",
    "bd_lyrics = pd.read_csv('/Users/Chandni/Documents/Education/NCSU/MSA/Case_Studies/lyrics_processed.csv', usecols=fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cretae wordcloud\n",
    "bd_wordcloud = WordCloud().generate(' '.join(bd_lyrics['processed_text']))\n",
    "\n",
    "# manually pass computed frequencies of words to cloud \n",
    "# WordCloud.generate_from_frequencies\n",
    "\n",
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "bd_wordcloud_1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(bd_lyrics['processed_text']))\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(bd_wordcloud_1, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the image in the img folder:\n",
    "bd_wordcloud_1.to_file(\"~/Case_Studies/bd_wordcloud_V1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more required packages \n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# need year for sentiment analysis and trend analysis \n",
    "# extract year from release date column \n",
    "# convert back csv to dataframe for year extraction and year concatenations\n",
    "bd_lyrics = pd.read_csv('~/Case_Studies/lyrics_processed.csv')\n",
    "lyrics_df = pd.DataFrame(bd_lyrics)\n",
    "\n",
    "lyrics_df.shape\n",
    "\n",
    "# need year for sentiment analysis and trend analysis \n",
    "# extract year from release date column \n",
    "# convert back csv to dataframe for year extraction and year concatenations\n",
    "lyrics_df['release_year'] = pd.DatetimeIndex(lyrics_df['release_date']).year\n",
    "lyrics_df.head()\n",
    "\n",
    "# create csv to check\n",
    "\n",
    "# check tail also since dates provided in year format only  \n",
    "lyrics_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group all processed text (lyrics that have been processed) for every year in a separate row\n",
    "# i.e. merge rows for same years \n",
    "\n",
    "# select only two columns from df for further analysis\n",
    "# select two columns \n",
    "text_date_df = lyrics_df[['processed_text', 'release_year']] \n",
    "text_date_df.head()\n",
    "# text_date_df.tail()\n",
    "\n",
    "text_date_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and check with a small subset of data first - select first two rows only\n",
    "test_df = text_date_df[0:3]\n",
    "\n",
    "#test_df.shape\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge rows for same years for test data set\n",
    "grouped_test_df = test_df.groupby(['release_year'], as_index = False).agg({'processed_text': ' ' .join})\n",
    "\n",
    "#grouped_test_df.shape\n",
    "grouped_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write test data to csv to check\n",
    "grouped_test_df.to_csv('~/Case_Studies/grouped_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rows for same years for entire data set\n",
    "grouped_text_date_df = text_date_df.groupby(['release_year'], as_index = False).agg({'processed_text': ' ' .join})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "grouped_text_date_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to ENTIRE date set to csv to check\n",
    "grouped_text_date_df.to_csv('~/Case_Studies/grouped_all_data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_text_date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import more packages \n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SentimentIntensityAnalyzer class \n",
    "#from vaderSentiment.vaderSentiment module. \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check only for positive sentiment first \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for index, row in grouped_text_date_df.iterrows():\n",
    "    grouped_text_date_df.at[index, 'positive'] = sid.polarity_scores(grouped_text_date_df.loc[index, 'processed_text'])['pos']\n",
    "    \n",
    "\n",
    "lyrics_year = grouped_text_date_df.groupby(['release_year'])['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for positive and negative sentiment and create final dataset for sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# create new df to prevent error\n",
    "new_grouped_data = grouped_text_date_df\n",
    "\n",
    "for index, row in new_grouped_data.iterrows():\n",
    "    new_grouped_data.at[index, 'positive'] = sid.polarity_scores(new_grouped_data.loc[index, 'processed_text'])['pos']\n",
    "    new_grouped_data.at[index, 'negative'] = sid.polarity_scores(new_grouped_data.loc[index, 'processed_text'])['neg']\n",
    "    \n",
    "\n",
    "lyrics_year = grouped_text_date_df.groupby(['release_year'])['positive', 'negative']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new data sets shape for this should include the positive and negative columns \n",
    "new_grouped_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to this sentiment analysis data set to csv for creating visualization in tableau\n",
    "new_grouped_data.to_csv('~/Case_Studies/Spotify_Senti_Analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
